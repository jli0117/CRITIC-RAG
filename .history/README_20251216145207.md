# CRITIC-RAG: Verification-Enhanced Retrieval-Augmented Generation for Medical QA

CRITIC-RAG is a novel framework designed to enhance the factual accuracy and reliability of large language models (LLMs) in the domain of medical question answering. By integrating an instruction-tuned verifier throughout the RAG pipeline, CRITIC-RAG systematically filters irrelevant information, verifies answer grounding, and enables structured multi-step reasoning.

## ğŸ§  Key Features

- **Retrieve-on-Demand**: Determines whether external retrieval is necessary based on query complexity.
- **Evidence Filtering**: Filters retrieved content using a verifier model to ensure relevance.
- **Self-Consistency via Clustering**: Clusters evidence and samples diverse subsets for multi-path reasoning.
- **Groundedness Verification**: Selects the most reliable answer based on evidence alignment scores.
- **Plug-and-Play**: Designed for easy integration with a variety of LLMs without major architectural changes.


## ğŸ§ª Benchmarks Used

- **[MedQA](https://arxiv.org/abs/2309.06024)**  
  A large-scale dataset with over 12,000 multiple-choice questions from USMLE-style exams covering various medical subfields.

  > [Github](https://github.com/jind11/MedQA)

- **[MMLU-Med (Medical Subset of MMLU)](https://arxiv.org/abs/2009.03300)**  
  Part of the Massive Multitask Language Understanding benchmark. Includes:
  - anatomy  
  - clinical knowledge  
  - college biology  
  - college medicine  
  - medical genetics  
  - professional medicine  
  > [HuggingFace](https://huggingface.co/datasets/cais/mmlu)

- **[PubMedQA](https://arxiv.org/abs/1909.06146)**  
  A large-scale biomedical question answering dataset collected from PubMed abstracts.
  > [GitHub](https://github.com/pubmedqa/pubmedqa)

- **[LiveQA (TREC 2017 Medical QA)](https://trec.nist.gov/data/medical.html)**  
  Real-world consumer health questions from the TREC LiveQA medical track.
  > [GitHub](https://github.com/abachaa/LiveQA_MedicalTask_TREC2017)

- **[MedicationQA](https://aclanthology.org/2023.acl-long.610/)**  
  Open-domain dataset of 4,151 questions focused on medication usage, dosage, interactions, and side effects.
  > [Github](https://github.com/abachaa/Medication_QA_MedInfo2019)

---

## ğŸ—ï¸ Architecture

Below is a visual overview of the CRITIC-RAG pipeline, highlighting its multi-stage verification components:

![CRITIC-RAG Pipeline](img/critic-rag-pipeline.png)


## ğŸ“ Training Examples

This section illustrates the prompt template used for instruction-tuning the CRITIC-RAG verifier, which consists of three main steps:

1. **Retrieve-on-Demand Verification**: Determines whether external retrieval is necessary based on query complexity.
2. **Evidence Filtering for Relevance Verification**: Filters retrieved content to ensure relevance.
3. **Self-Consistency and Groundedness Verification**: Verifies that the selected answer is grounded in the most relevant evidence.

![Prompt Template for Instruction-Tuning](img/prompt.png)


## ğŸ“‘ Case Study

We present the list of retrieved evidence documents used in the case study discussed in the manuscript. The table below outlines the full list, which includes the top-10 retrieved passages, those retained after evidence filtering, and the final clustered subsets that contributed to the selected answer.

![Retrieved evidences for case study](img/case_study.png)




## ğŸ”— Code References

This project builds on ideas and code from the following repositories:

- [KALMV](https://github.com/JinheonBaek/KALMV): Knowledge-Augmented Language Model Verification (EMNLP 2023)  
- [Self-BioRAG](https://github.com/dmis-lab/self-biorag): Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models (Bioinformatics, 2024)  


